name: GATK MarkDuplicatesSpark
url: https://gatk.broadinstitute.org/hc/en-us/articles/360050814112-MarkDuplicatesSpark
description: |
  Spark implementation of Picard MarkDuplicates that allows the tool to be run in parallel on multiple cores on a local machine or multiple machines on a Spark cluster while still matching the output of the non-Spark Picard version of the tool. Since the tool requires holding all of the readnames in memory while it groups read information, machine configuration and starting sort-order impact tool performance.
authors:
  - Filipe G. Vieira
input:
  - Path to bam file, this must be the only input element in the input file list
output:
  - bam: Path to bam file with marked or removed duplicates
  - metrics: Optional metrics file
params:
  - extra: The `extra` param allows for additional program arguments for markduplicatesspark.
  - java_opts: The `java_opts` param allows for additional arguments to be passed to the java compiler, e.g. "-Xmx4G" for one, and "-Xmx4G -XX:ParallelGCThreads=10" for two options.
  - spark_runner: The `spark_runner` param = "LOCAL"|"SPARK"|"GCS" allows to set the spark_runner. Set the parameter to "LOCAL" or don't set it at all to run on local machine.
  - spark_master: The `spark_master` param allows to set the URL of the Spark Master to submit the job. Set to "local[number_of_cores]" for local execution. Don't set it at all for local execution with number of cores determined by snakemake.
  - spark_extra: The `spark_extra` param allows for additional spark arguments.
